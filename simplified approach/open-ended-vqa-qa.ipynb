{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9504399,"sourceType":"datasetVersion","datasetId":5784662}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Creating the Dataset class and data loaders ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport pickle\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import CLIPImageProcessor, GPT2Tokenizer\nfrom transformers import TimesformerConfig, TimesformerModel\n\nclass VQARADDataset(Dataset):\n    def __init__(self, pickle_dataset, gif_feat_dic):\n        \"\"\"\n        Initializes the VQARADDataset object by loading a preprocessed dataset and corresponding GIF features.\n\n        Args:\n            pickle_dataset (str): Path to the pickled dataset file that contains question features, answers, and other metadata.\n            gif_feat_dic (str): Path to the pickled dictionary containing pre-extracted GIF embeddings.\n\n        Attributes:\n            tgif_frame (pd.DataFrame): DataFrame containing the dataset information (questions, answers, and features).\n            gif_feat_dict (dict): Dictionary mapping GIF names to their feature embeddings.\n            gpt2_tokenizer (GPT2Tokenizer): Tokenizer for tokenizing the answers using the GPT-2 tokenizer.\n        \"\"\"\n\n        with open(gif_feat_dic, 'rb') as f:\n            gif_feat_dict = pickle.load(f)\n        self.tgif_frame = pd.read_pickle(pickle_dataset)\n        self.gif_feat_dict = gif_feat_dict\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n\n    def __len__(self):\n        return len(self.tgif_frame)\n\n    def __getitem__(self, idx):\n        gif_name = self.tgif_frame.iloc[idx, 1]+'.gif'\n        question_features = self.tgif_frame.iloc[idx, 4]\n        answers = self.tgif_frame.iloc[idx, 3] + \" <END>\"\n        answer_id = self.gpt2_tokenizer(answers, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=37).input_ids.squeeze(0)  # One extra for the end token.\n        gif_features = self.gif_feat_dict[gif_name]\n        \n        sample = {'gif_embeddings': gif_features, 'question_embeddings': question_features, 'answer': answer_id}\n        \n        return sample\n\n\ndef get_loaders(pickle_dataset='/kaggle/input/cleaned-gif-embeddings/questions_with_gpt2_embeddings_using_gpu_cleaned.pkl', gif_feat_dic='/kaggle/input/cleaned-gif-embeddings/combined_file.pkl', batch_size=32, split_ratio=(0.9, 0.09, 0.01)):\n    \"\"\"\n    Returns DataLoaders for training, validation, and test splits.\n\n    Args:\n        pickle_dataset (str): Path to the pickled dataset file.\n        gif_feat_dic (str): Path to the pickled dictionary containing GIF embeddings.\n        batch_size (int): The batch size for the DataLoader.\n        split_ratio (tuple): A tuple of three values representing the ratios for training, validation, and test splits. Must sum to 1.\n\n    Returns:\n        tuple: A tuple containing:\n            - train_loader (DataLoader): DataLoader for the training split.\n            - val_loader (DataLoader): DataLoader for the validation split.\n            - test_loader (DataLoader): DataLoader for the test split.\n    \"\"\"\n\n\n    assert sum(split_ratio) == 1, \"Split ratios should sum to 1.\"\n\n    dataset = VQARADDataset(pickle_dataset, gif_feat_dic) \n    \n    total_size = len(dataset)\n    train_size = int(split_ratio[0] * total_size)\n    val_size = int(split_ratio[1] * total_size)\n    test_size = total_size - train_size - val_size\n\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T22:14:28.939303Z","iopub.execute_input":"2024-09-28T22:14:28.939795Z","iopub.status.idle":"2024-09-28T22:14:28.957162Z","shell.execute_reply.started":"2024-09-28T22:14:28.939729Z","shell.execute_reply":"2024-09-28T22:14:28.955882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Model class","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import CLIPProcessor, CLIPModel, GPT2LMHeadModel\nfrom typing import Tuple\nfrom PIL import Image\n\nclass VQAModel(nn.Module):\n    def __init__(self):\n        \"\"\"\n        Initializes the VQAModel by loading a pretrained GPT-2 model and its corresponding tokenizer.\n        \n        Components:\n        - `gpt2_model`: A pretrained DistilGPT2 model to generate textual answers.\n        - `project_down`: A linear layer to project concatenated image and question features into the appropriate size for GPT-2.\n        - `gpt2_tokenizer`: The tokenizer associated with the DistilGPT2 model, used for decoding the generated tokens.\n        \"\"\"\n        super(VQAModel, self).__init__()\n        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")  # Load GPT-2 model for answer generation\n        self.project_down = nn.Linear(768*2, 768)  # Linear layer to combine image and question embeddings into 768 dimensions\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")  # Tokenizer for GPT-2 model\n\n    def forward(self, image_features: torch.Tensor, question_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the VQA model.\n        \n        Args:\n        - `image_features`: Tensor of features extracted from an image.\n        - `question_features`: Tensor of features extracted from a text question.\n        \n        Returns:\n        - `logits`: Output logits from the GPT-2 model representing the next token predictions.\n        - `generated_sequence`: The sequence of tokens generated by the GPT-2 model.\n        \"\"\"\n        # Concatenate image and question features along the last dimension\n        combined_features = torch.cat((image_features, question_features), dim=-1)\n        \n        # Project down the concatenated features to match GPT-2's input size (768 dimensions)\n        combined_features = self.project_down(combined_features)\n        combined_features = combined_features.unsqueeze(1)  # Add a sequence dimension for GPT-2 input\n\n        # Pass the features through the GPT-2 model to get logits\n        outputs = self.gpt2_model(inputs_embeds=combined_features)\n        logits = outputs.logits  # Logits for the next token predictions\n        \n        # Tokenize and encode the end-of-sequence token\n        eos_token_id = self.gpt2_tokenizer.encode(\"<END>\", add_prefix_space=True)[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T22:14:57.816036Z","iopub.execute_input":"2024-09-28T22:14:57.816477Z","iopub.status.idle":"2024-09-28T22:14:57.829406Z","shell.execute_reply.started":"2024-09-28T22:14:57.816444Z","shell.execute_reply":"2024-09-28T22:14:57.827996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the training loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Initialize GPT-2 tokenizer for later decoding of model outputs\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n\n# Hyperparameters\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 1e-4\n\n# File paths for saving model checkpoints at different stages\nMODEL_PATH = '/kaggle/working/best_model.pth'\nMODEL_PATH_2 = '/kaggle/working/25_model.pth'\nMODEL_PATH_3 = '/kaggle/working/50_model.pth'\n\n# Check for CUDA availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Get data loaders for training and validation\ntrain_loader, val_loader, _ = get_loaders(batch_size=BATCH_SIZE)\n\nmodel = VQAModel().to(device)\n\n# Use DataParallel to parallelize the model across multiple GPUs if available\nmodel = torch.nn.DataParallel(model)\n\n# Loss function (cross-entropy) and optimizer (Adam)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ndef train(model, dataloader, criterion, optimizer, device):\n    \"\"\"\n    Train the model for one epoch.\n\n    Args:\n    - model (nn.Module): The VQA model.\n    - dataloader (DataLoader): DataLoader for training data.\n    - criterion (nn.Module): Loss function (cross-entropy).\n    - optimizer (optim.Optimizer): Optimizer for training (Adam).\n    - device (torch.device): Device to perform training (CPU or GPU).\n\n    Returns:\n    - avg_loss (float): Average training loss over the epoch.\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=\"Training\"):\n        # Unpack input data\n        image_features, question_features, answers = batch['gif_embeddings'], batch['question_embeddings'], batch['answer']\n        image_features, question_features, answers = image_features.to(device), question_features.to(device), answers.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        logits, _ = model(image_features, question_features)\n\n        # Adjust sequence lengths between predictions and ground truth answers\n        if logits.size(1) < answers.size(1):\n            answers = answers[:, :logits.size(1)]\n        elif logits.size(1) > answers.size(1):\n            logits = logits[:, :answers.size(1)]\n\n        # Reshape for the loss function\n        logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n        answers_reshaped = answers.contiguous().view(-1)\n\n        # Compute loss and perform backpropagation\n        loss = criterion(logits_reshaped, answers_reshaped)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    avg_loss = running_loss / len(dataloader)\n    return avg_loss\n\ndef compute_bleu(reference, hypothesis):\n    \"\"\"\n    Compute BLEU score between reference and hypothesis.\n\n    Args:\n    - reference (str): The ground truth reference sentence.\n    - hypothesis (str): The predicted hypothesis sentence.\n\n    Returns:\n    - bleu (float): BLEU score between 0 and 1.\n    \"\"\"\n    reference = reference.split()\n    hypothesis = hypothesis.split()\n\n    references = [reference] \n\n    smoothing = SmoothingFunction().method1\n    bleu = sentence_bleu(references, hypothesis, smoothing_function=smoothing)\n\n    return bleu\n\ndef validate(model, dataloader, criterion, device):\n    \"\"\"\n    Validate the model and compute loss and BLEU score.\n\n    Args:\n    - model (nn.Module): The VQA model.\n    - dataloader (DataLoader): DataLoader for validation data.\n    - criterion (nn.Module): Loss function (cross-entropy).\n    - device (torch.device): Device to perform validation (CPU or GPU).\n\n    Returns:\n    - avg_loss (float): Average validation loss.\n    - average_bleu (float): Average BLEU score for the validation set.\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    total = 0\n    total_bleu = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            image_features, question_features, answers = batch['gif_embeddings'], batch['question_embeddings'], batch['answer']\n            image_features, question_features, answers = image_features.to(device), question_features.to(device), answers.to(device)\n\n            # Forward pass\n            logits, gen_seq = model(image_features, question_features)\n\n            # Decode generated token sequences to strings\n            generated_answers = [tokenizer.decode(g, skip_special_tokens=True) for g in gen_seq]\n\n            # Decode reference answers\n            reference_answers = [tokenizer.decode(a, skip_special_tokens=True) for a in answers]\n\n            # Compute BLEU score for each reference/predicted pair\n            for true, generated in zip(reference_answers, generated_answers):\n                total_bleu += compute_bleu(true.lower(), generated.lower())\n\n            # Adjust sequence lengths and compute loss\n            if logits.size(1) < answers.size(1):\n                answers = answers[:, :logits.size(1)]\n            elif logits.size(1) > answers.size(1):\n                logits = logits[:, :answers.size(1)]\n\n            logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n            answers_reshaped = answers.contiguous().view(-1)\n\n            loss = criterion(logits_reshaped, answers_reshaped)\n            running_loss += loss.item()\n\n            total += answers_reshaped.size(0)\n\n    avg_loss = running_loss / len(dataloader)\n    average_bleu = total_bleu / total if total > 0 else 0\n\n    return avg_loss, average_bleu\n\n# Training loop\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    # Train the model and compute loss\n    train_loss = train(model, train_loader, criterion, optimizer, device)\n    \n    # Validate the model and compute loss and BLEU score\n    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n\n    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.2f}%\")\n\n    # Save the best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"Model saved to {MODEL_PATH}\")\n    \n    # Save the model at epoch 25\n    if epoch == 24:\n        torch.save(model.state_dict(), MODEL_PATH_2)\n        print(f\"Model saved to {MODEL_PATH_2}\")\n    \n    # Save the model at epoch 50\n    if epoch == 49:\n        torch.save(model.state_dict(), MODEL_PATH_3)\n        print(f\"Model saved to {MODEL_PATH_3}\")\n\nprint(\"Training complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T22:15:01.607621Z","iopub.execute_input":"2024-09-28T22:15:01.608437Z"},"trusted":true},"execution_count":null,"outputs":[]}]}