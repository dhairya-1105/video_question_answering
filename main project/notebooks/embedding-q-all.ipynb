{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9504374,"sourceType":"datasetVersion","datasetId":5779610},{"sourceId":9522747,"sourceType":"datasetVersion","datasetId":5794664}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom tqdm import tqdm  # For progress bar\n\n# Check if CUDA is available and the number of GPUs\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of GPUs available: {num_gpus}\")\n\n# Load pre-trained GPT-2 model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\nnew_pad_token = \"[PAD]\"\nif new_pad_token not in tokenizer.get_vocab():\n    tokenizer.add_special_tokens({'pad_token': new_pad_token})\n    print(f\"Added new pad token: {new_pad_token}\")\nmodel = GPT2Model.from_pretrained(\"openai-community/gpt2-medium\")\nmodel.resize_token_embeddings(len(tokenizer))  # Resize embeddings to accommodate new pad token\n\n# Move the model to the device\nmodel.to(device)\n\n# If multiple GPUs are available, wrap the model with DataParallel\nif num_gpus > 1:\n    model = torch.nn.DataParallel(model)\n    print(\"Model wrapped with DataParallel for multi-GPU usage.\")\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Function to generate embeddings for a batch of concatenated questions and answers\ndef generate_embeddings_batch(texts, tokenizer, model, device, max_length=64):\n    # Tokenize the batch of texts\n    inputs = tokenizer(\n        texts,\n        return_tensors='pt',\n        max_length=max_length,\n        truncation=True,\n        padding='max_length'\n    )\n    # Move inputs to the device\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        # Get the last hidden state\n        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n        # Move to CPU and convert to numpy\n        embeddings = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    \n    return embeddings\n\n# Parameters for batching\nbatch_size = 128  # Adjust based on your GPU memory\n\n# Load your DataFrame\ndf = pd.read_csv(\"/kaggle/input/tgif-qna-descriptions-38k/updated_final_dataframe_complete.csv\")\n\nprint(f\"DataFrame shape: {df.shape}\")\n\nembeddings = []\n\n# Iterate over the DataFrame in batches\nfor i in range(0, len(df), batch_size):\n    batch_df = df.iloc[i:i+batch_size]\n    # Concatenate question and answer with the separator token\n    batch_texts = batch_df['question']\n    # Generate embeddings for the batch\n    batch_embeddings = generate_embeddings_batch(batch_texts.tolist(), tokenizer, model, device)\n    embeddings.extend(batch_embeddings)\n\n# Add the embeddings to the DataFrame\ndf['question_embedding'] = embeddings\n\n# Save the DataFrame with embeddings\ndf.to_pickle(f'updated_final_df_with_q_embeddings.pkl')\n    \nprint(df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **** THE GIVEN CODE SHOWS THE PROCESS OF GENERATING QUESTION EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom tqdm import tqdm  # For displaying progress bars during iteration\n\n# -----------------------------------------------------------------------------------\n# Configuration and Setup\n# -----------------------------------------------------------------------------------\n\n# Determine the computing device: use GPU if available, else fallback to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Get the number of GPUs available\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of GPUs available: {num_gpus}\")\n\n# -----------------------------------------------------------------------------------\n# Loading the Pre-trained GPT-2 Model and Tokenizer\n# -----------------------------------------------------------------------------------\n\n# Initialize the GPT-2 tokenizer from the specified pre-trained model\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n\n# Define a new padding token\nnew_pad_token = \"[PAD]\"\n\n# Add the new padding token to the tokenizer's vocabulary if it's not already present\nif new_pad_token not in tokenizer.get_vocab():\n    tokenizer.add_special_tokens({'pad_token': new_pad_token})\n    print(f\"Added new pad token: {new_pad_token}\")\n\n# Load the pre-trained GPT-2 model\nmodel = GPT2Model.from_pretrained(\"openai-community/gpt2-medium\")\n\n# Resize the model's token embeddings to accommodate the new padding token\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Move the model to the designated computing device (GPU or CPU)\nmodel.to(device)\n\n# If multiple GPUs are available, enable DataParallel for parallel processing\nif num_gpus > 1:\n    model = torch.nn.DataParallel(model)\n    print(\"Model wrapped with DataParallel for multi-GPU usage.\")\n\n# Set the model to evaluation mode to disable dropout and other training-specific layers\nmodel.eval()\n\n# -----------------------------------------------------------------------------------\n# Function Definitions\n# -----------------------------------------------------------------------------------\n\ndef generate_embeddings_batch(texts, tokenizer, model, device, max_length=64):\n    \"\"\"\n    Generates embeddings for a batch of texts using the GPT-2 model.\n\n    Args:\n        texts (list of str): List of text strings (e.g., questions) to generate embeddings for.\n        tokenizer (GPT2Tokenizer): Tokenizer corresponding to the GPT-2 model.\n        model (GPT2Model): Pre-trained GPT-2 model for generating embeddings.\n        device (torch.device): The device (CPU or GPU) to perform computations on.\n        max_length (int, optional): Maximum sequence length for tokenization. Defaults to 64.\n\n    Returns:\n        numpy.ndarray: Array of embeddings with shape (batch_size, hidden_size).\n    \"\"\"\n    # Tokenize the batch of texts with padding and truncation\n    inputs = tokenizer(\n        texts,\n        return_tensors='pt',\n        max_length=max_length,\n        truncation=True,\n        padding='max_length'\n    )\n    \n    # Move tokenized inputs to the designated device\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    with torch.no_grad():  # Disable gradient calculations for efficiency\n        outputs = model(**inputs)\n        # Extract the last hidden state from the model's output\n        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n        # Compute the mean across the sequence length to obtain fixed-size embeddings\n        embeddings = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    \n    return embeddings\n\n# -----------------------------------------------------------------------------------\n# Parameters and Data Loading\n# -----------------------------------------------------------------------------------\n\n# Define the batch size for processing data in chunks\nbatch_size = 128  # Adjust based on available GPU memory\n\n# Load the dataset from a CSV file into a Pandas DataFrame\ndf = pd.read_csv(\"/kaggle/input/tgif-qna-descriptions-38k/updated_final_dataframe_complete.csv\")\n\nprint(f\"DataFrame shape: {df.shape}\")\n\n# Initialize a list to store the generated embeddings\nembeddings = []\n\n# -----------------------------------------------------------------------------------\n# Generating Embeddings for the Dataset\n# -----------------------------------------------------------------------------------\n\n# Iterate over the DataFrame in batches to efficiently process large datasets\nfor i in tqdm(range(0, len(df), batch_size), desc=\"Generating Embeddings\"):\n    # Select a batch of rows from the DataFrame\n    batch_df = df.iloc[i:i+batch_size]\n    \n    # Extract the 'question' column as the text input for embedding\n    batch_texts = batch_df['question']\n    \n    # Generate embeddings for the current batch of texts\n    batch_embeddings = generate_embeddings_batch(batch_texts.tolist(), tokenizer, model, device)\n    \n    # Append the generated embeddings to the main list\n    embeddings.extend(batch_embeddings)\n\n# -----------------------------------------------------------------------------------\n# Saving the Embeddings\n# -----------------------------------------------------------------------------------\n\n# Add the generated embeddings as a new column in the DataFrame\ndf['question_embedding'] = embeddings\n\n# Save the updated DataFrame with embeddings to a pickle file for efficient storage and retrieval\ndf.to_pickle('updated_final_df_with_q_embeddings.pkl')\n    \n# Display the first few rows of the updated DataFrame to verify the embeddings\nprint(df.head())\n","metadata":{},"execution_count":null,"outputs":[]}]}