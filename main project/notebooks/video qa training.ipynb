{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9522747,"sourceType":"datasetVersion","datasetId":5794664},{"sourceId":199032368,"sourceType":"kernelVersion"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --quiet av sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport av\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport torch\nimport requests\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, TimesformerModel\nfrom transformers import GPT2Tokenizer\n\nclass VQAGIFDataset(Dataset):\n    def __init__(self, pickle_dataset): \n        \"\"\"\n        Args:\n            pickle dataset (string): Path to the pickle file with urls, questions, answers and question text embeddings.\n        \n        Returns a dict with the pixel values, question embeddings and answer tokens\n        \"\"\"\n        self.tgif_frame = pd.read_pickle(pickle_dataset)\n        \n        self.preprocess = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n        new_pad_token = \"[PAD]\"\n        if new_pad_token not in self.gpt2_tokenizer.get_vocab():\n            self.gpt2_tokenizer.add_special_tokens({'pad_token': new_pad_token})\n\n    def __len__(self):\n        return len(self.tgif_frame)\n    \n    def download_gif(self, gif_url, output_path):\n        response = requests.get(gif_url, stream=True)\n        if response.status_code == 200:\n            with open(output_path, 'wb') as f:\n                f.write(response.content)\n            return True\n        return False\n\n    def read_video_pyav(self, container, indices):\n        '''\n        Decode the video with PyAV decoder.\n        '''\n        frames = []\n        container.seek(0)\n        start_index = indices[0]\n        end_index = indices[-1]\n        for i, frame in enumerate(container.decode(video=0)):\n            if i > end_index:\n                break\n            if i >= start_index and i in indices:\n                frames.append(frame)\n        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n    def sample_frame_indices(self, clip_len, frame_sample_rate, seg_len):\n        '''\n        Sample a given number of frame indices from the video.\n        '''\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n    \n    def process_gifs(self, gif_url):\n        '''\n        Gets the gif file from the url provided, reads the video file and returns a list of stacked frames\n        Args: gif_url: url to the gif\n        '''\n        embeddings_dict = {}\n        gifs_checked = 0\n        true_gifs = []\n        download_folder = \"/kaggle/working/gifs\"\n\n        # Create the download folder if it doesn't exist\n        if not os.path.exists(download_folder):\n            os.makedirs(download_folder)\n\n        gif_name = gif_url.split('/')[-1]\n        gif_path = os.path.join(download_folder, gif_name)\n\n        # Download the GIF\n        if self.download_gif(gif_url, gif_path):\n            gifs_checked += 1\n\n            container = av.open(gif_path)\n            total_frames = container.streams.video[0].frames\n\n            # Sample 8 frames from the GIF and process the video\n            indices = self.sample_frame_indices(clip_len=8, frame_sample_rate=0.5, seg_len=total_frames)\n            video = self.read_video_pyav(container, indices)\n\n            # Close the container to release any held resources\n            container.close()\n\n            # Delete the GIF file after processing\n            if os.path.exists(gif_path):\n                os.remove(gif_path)  # Deletes the downloaded GIF file\n\n        return list(video)\n\n    def __getitem__(self, idx):\n        # Gets the GIF features\n        gif_url = self.tgif_frame.iloc[idx, 0]\n        video_list = self.process_gifs(gif_url)\n        inputs = self.preprocess(video_list, return_tensors=\"pt\")\n        pixel_values = inputs.get('pixel_values')\n        pixel_values = pixel_values.squeeze(0)\n        \n        # Gets the question features\n        question_features = self.tgif_frame.iloc[idx, 4]\n        \n        # Tokenizes the answers\n        answers = self.tgif_frame.iloc[idx, 3] + \" <END>\"\n        answer_id = self.gpt2_tokenizer(answers, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=37).input_ids.squeeze(0)  # One extra for the end token.\n        \n        sample = {'processed_gifs': pixel_values, 'question_embeddings': question_features, 'answer': answer_id}\n        \n        return sample\n\n\ndef get_loaders(pickle_dataset='/kaggle/input/embedding-q-all/updated_final_df_with_q_embeddings.pkl', batch_size=16, split_ratio=(0.9, 0.09, 0.01)):\n    \"\"\"\n    Returns training, validation, and test data loaders.\n    Args:\n        pickle_dataset (string): Path to the pickle file.\n        batch_size (int): Batch size for DataLoader.\n        transform (callable, optional): Optional transform to be applied on image.\n        split_ratio (tuple): Ratios for train, val, and test split. They should sum to 1.\n    \"\"\"\n\n    assert sum(split_ratio) == 1, \"Split ratios should sum to 1.\"\n\n    dataset = VQAGIFDataset(pickle_dataset)\n    \n    total_size = len(dataset)\n    train_size = int(split_ratio[0] * total_size)\n    val_size = int(split_ratio[1] * total_size)\n    test_size = total_size - train_size - val_size\n    \n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n    torch.save(test_dataset, '/kaggle/working/test_dataset.pth')\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nclass CrossAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(CrossAttention, self).__init__()\n        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n    \n    def forward(self, query, key, value):\n        # Apply attention where the query is the question features and key, value are the video features\n        attn_output, _ = self.attn(query, key, value)\n        return attn_output\n\nclass VQAModel(nn.Module):\n    def __init__(self):\n        super(VQAModel, self).__init__()\n        \n        # For GIF Features\n        self.gif_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n        self.Timesformer_model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n        \n        # Cross attention and resizing functions\n        self.cross_attention = CrossAttention(embed_dim=768, num_heads=8)\n        self.question_projection = nn.Linear(1024, 768)\n        self.project_down = nn.Linear(768 * 2, 1024)\n        \n        # GPT2-Medium\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n        new_pad_token = \"[PAD]\"\n        if new_pad_token not in self.gpt2_tokenizer.get_vocab():\n            self.gpt2_tokenizer.add_special_tokens({'pad_token': new_pad_token})\n        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-medium\")\n        self.gpt2_model.resize_token_embeddings(len(self.gpt2_tokenizer))  # Resize embeddings to accommodate new pad token\n        self.gpt2_model.config.pad_token_id = self.gpt2_tokenizer.get_vocab()[new_pad_token]\n        \n        \n    def forward(self, processed_gif, question_features):\n        \n        # Gets the video embeddings\n        outputs = self.Timesformer_model(processed_gif)\n        image_features = outputs.last_hidden_state.mean(dim=1).unsqueeze(1)\n        \n        # Resizes the question features\n        question_features = self.question_projection(question_features)\n        question_features = question_features.unsqueeze(1)\n\n        # Applying cross-attention (text features as query, video features as key/value)\n        question_query = question_features.permute(1, 0, 2)  # (question_seq_len, batch_size, embed_dim)\n        image_key_value = image_features.permute(1, 0, 2)    # (image_seq_len, batch_size, embed_dim)\n        \n        cross_attended_features = self.cross_attention(query=question_query, key=image_key_value, value=image_key_value)\n        cross_attended_features = cross_attended_features.permute(1, 0, 2)  # back to (batch_size, question_seq_len, embed_dim)  \n        \n        # Concatenating Projecting the combined features down to match GPT-2's expected embedding size\n        combined_features = torch.cat((cross_attended_features, question_features), dim=-1)\n        combined_features = self.project_down(combined_features)\n        \n        # Pass the combined features to GPT-2\n        outputs = self.gpt2_model(inputs_embeds=combined_features)\n        logits = outputs.logits\n        \n        # Generation of the output sequence using GPT-2\n        eos_token_id = self.gpt2_tokenizer.encode(\"<END>\", add_prefix_space=True)[0]\n        generated_sequence = self.gpt2_model.generate(\n                                                        inputs_embeds=combined_features, \n                                                        max_length=16, \n                                                        pad_token_id=self.gpt2_model.config.pad_token_id, \n                                                        eos_token_id=eos_token_id,\n                                                        repetition_penalty = 1.2,\n                                                        top_k = 50,\n                                                        top_p = 0.9,\n                                                        num_beams=5,  # Number of beams\n                                                        early_stopping=True,\n                                                        no_repeat_ngram_size=2,\n                                                        temperature=0.7\n                                                    )\n        \n        return logits, generated_sequence\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm \nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\nBATCH_SIZE = 16\nEPOCHS = 20\nLEARNING_RATE = 1e-4\nMODEL_PATH = '/kaggle/working/best_model.pth'\nMODEL_PATH_2 = '/kaggle/working/10_model.pth'\nMODEL_PATH_3 = '/kaggle/working/15_model.pth'\ntest_data_path = '/kaggle/working/test_data.csv'\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_loader, val_loader, test_loader = get_loaders(batch_size=BATCH_SIZE)\nmodel = VQAModel().to(device)\nmodel = torch.nn.DataParallel(model)\nval_model = SentenceTransformer('all-MiniLM-L6-v2')\ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for batch in tqdm(train_loader, desc=\"Training\"): \n        \n        images, question_features, answers = batch['processed_gifs'], batch['question_embeddings'], batch['answer']\n        images = images.to(device)\n        question_features = question_features.to(device)\n        answers = answers.to(device)\n        \n        optimizer.zero_grad()\n        logits, _ = model(images, question_features)\n        if logits.size(1) < answers.size(1):\n            answers = answers[:, :logits.size(1)]\n        elif logits.size(1) > answers.size(1):\n            logits = logits[:, :answers.size(1)]\n        logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n        answers_reshaped = answers.contiguous().view(-1)\n        \n        loss = criterion(logits_reshaped, answers_reshaped)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    avg_loss = running_loss / len(dataloader)\n    return avg_loss\n\ndef validate(model, dataloader, criterion, tokenizer, val_model, device):\n    model.eval()\n    running_loss = 0.0\n    total = 0\n    similarity_scores = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            images, question_features, answer_tokens = batch['processed_gifs'], batch['question_embeddings'], batch['answer']\n            images = images.to(device)\n            question_features = question_features.to(device)\n            answer_tokens = answer_tokens.to(device)\n\n            logits, gen_seq = model(images, question_features)\n            \n            reference_answers = [tokenizer.decode(a, skip_special_tokens=True) for a in answer_tokens]\n            generated_answers = [tokenizer.decode(g, skip_special_tokens=True) for g in gen_seq]\n            \n            reference_embeddings = val_model.encode(reference_answers, convert_to_tensor=False)\n            model_embeddings = val_model.encode(generated_answers, convert_to_tensor=False)\n\n            for ref_emb, mod_emb in zip(reference_embeddings, model_embeddings):\n                similarity = cosine_similarity([ref_emb], [mod_emb])[0][0]\n                similarity_scores.append(similarity)\n            \n            if logits.size(1) < answer_tokens.size(1):\n                answer_tokens = answer_tokens[:, :logits.size(1)]\n            elif logits.size(1) > answer_tokens.size(1):\n                logits = logits[:, :answer_tokens.size(1)]\n            logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n            answers_reshaped = answer_tokens.contiguous().view(-1)\n            total += answers_reshaped.size(0)\n\n            loss = criterion(logits_reshaped, answers_reshaped)\n            running_loss += loss.item()\n    \n    avg_loss = running_loss / len(dataloader)\n    average_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0\n\n    return avg_loss, average_similarity\n\n\nbest_val_loss = float('inf')\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    train_loss = train(model, train_loader, criterion, optimizer, device)\n    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n    \n    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.2f}%\")\n  \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"Model {epoch} saved to {MODEL_PATH}\")\n    elif epoch > 15:\n        epochs_no_improve += 1\n        if epochs_no_improve >= 5:\n            print('Early stopping!')\n            early_stop = True\n            break\n    \n    if epoch == 10:\n        torch.save(model.state_dict(), MODEL_PATH_2)\n        print(f\"Model saved to {MODEL_PATH_2}\")\n    \n    if epoch == 15:\n        torch.save(model.state_dict(), MODEL_PATH_3)\n        print(f\"Model saved to {MODEL_PATH_3}\")\n        \nprint(\"Training complete.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}