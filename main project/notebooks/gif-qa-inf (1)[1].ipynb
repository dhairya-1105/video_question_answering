{"cells":[{"cell_type":"markdown","metadata":{},"source":["# WRITING THE MODEL HERE FOR LOADING WEIGHTS"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:43:25.578815Z","iopub.status.busy":"2024-10-03T07:43:25.578222Z","iopub.status.idle":"2024-10-03T07:43:25.587952Z","shell.execute_reply":"2024-10-03T07:43:25.587015Z","shell.execute_reply.started":"2024-10-03T07:43:25.578775Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","class VQAModel(nn.Module):\n","    def __init__(self):\n","        super(VQAModel, self).__init__()\n","        self.project_down = nn.Linear(768*2, 768)   \n","        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n","        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n","\n","    def forward(self, image_features, question_features):\n","        \n","        # Concatenating features\n","        combined_features = torch.cat((image_features, question_features), dim=-1) \n","\n","        # Resizing features to match inpput dimensions\n","        combined_features = self.project_down(combined_features) # Resizing features to match inpput dimensions\n","        combined_features = combined_features.unsqueeze(1) \n","        \n","        # Generate outputs\n","        outputs = self.gpt2_model(inputs_embeds=combined_features)\n","        logits = outputs.logits\n","        eos_token_id = self.gpt2_tokenizer.encode(\"<END>\", add_prefix_space=True)[0]\n","        generated_sequence = self.gpt2_model.generate(inputs_embeds=combined_features, \n","                                                      max_length=2, \n","                                                      pad_token_id=eos_token_id, \n","                                                      repetition_penalty=5.7,\n","                                                      temperature=0.9,\n","                                                      eos_token_id=eos_token_id)\n","        return logits, generated_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:28:34.551485Z","iopub.status.busy":"2024-10-03T07:28:34.551124Z","iopub.status.idle":"2024-10-03T07:28:35.578822Z","shell.execute_reply":"2024-10-03T07:28:35.577819Z","shell.execute_reply.started":"2024-10-03T07:28:34.551449Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/552269125.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import torch  # PyTorch library for tensor computations and deep learning\n","\n","# -----------------------------------------------------------------------------------\n","# Model Loading and Preparation\n","# -----------------------------------------------------------------------------------\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","model = VQAModel().to(device)\n","print(\"VQAModel initialized and moved to device.\")\n","\n","# Define the path to the pre-trained model's state dictionary\n","model_path = '/kaggle/working/model_cpu.pth'\n","print(f\"Loading model state dictionary from: {model_path}\")\n","\n","# Load the state dictionary from the specified path\n","# torch.load handles loading the saved state dictionary\n","# If the model was saved on a GPU and you're loading it on a CPU, use map_location\n","state_dict = torch.load(model_path, map_location=device)\n","model.load_state_dict(state_dict)\n","print(\"Model state dictionary loaded successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:44:15.832087Z","iopub.status.busy":"2024-10-03T07:44:15.831693Z","iopub.status.idle":"2024-10-03T07:44:16.993361Z","shell.execute_reply":"2024-10-03T07:44:16.992493Z","shell.execute_reply.started":"2024-10-03T07:44:15.832046Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]}],"source":["import numpy as np\n","import pickle\n","\n","gif_feat_dic = \"/kaggle/input/cleaned-gif-embeddings/combined_file.pkl\"\n","pickle_dataset = \"/kaggle/input/cleaned-gif-embeddings/questions_with_gpt2_embeddings_using_gpu_cleaned.pkl\"\n","\n","with open(gif_feat_dic, 'rb') as f:\n","    gif_feat_dict = pickle.load(f)\n","tgif_frame = pd.read_pickle(pickle_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["# WRITING A FUNCTION TO GET THE VDEO EMBEDDINGS AND TEXT EMBEDDINGS "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:28:45.208080Z","iopub.status.busy":"2024-10-03T07:28:45.207330Z","iopub.status.idle":"2024-10-03T07:28:45.214488Z","shell.execute_reply":"2024-10-03T07:28:45.213319Z","shell.execute_reply.started":"2024-10-03T07:28:45.208036Z"},"trusted":true},"outputs":[],"source":["def getitem(idx):\n","    \"\"\"\n","    Retrieves a sample from the tgif_frame dataset given an index and returns the relevant features \n","    such as GIF embeddings, question features, and answers.\n","\n","    Args:\n","    idx (int): The index of the row in the tgif_frame dataset to retrieve the sample.\n","\n","    Returns:\n","    dict: A dictionary containing the following:\n","        - 'gif_embeddings': Precomputed GIF embeddings from gif_feat_dict.\n","        - 'question_embeddings': Tensor containing the question features.\n","        - 'answer': The corresponding answer for the question, appended with \"<END>\" token.\n","        - 'url': The URL of the GIF.\n","        - 'question': The textual question.\n","    \"\"\"\n","    # Gets question features\n","    question_features = torch.from_numpy(tgif_frame.iloc[idx, 4])\n","\n","    # Gets answers\n","    answers = tgif_frame.iloc[idx, 3]\n","\n","    # Gets the GIF features from the dictionary\n","    gif_name = tgif_frame.iloc[idx, 1] + '.gif'\n","    gif_features = gif_feat_dict[gif_name]\n","    gif_url = tgif_frame.iloc[idx, 0]\n","    question = tgif_frame.iloc[idx, 2]\n","\n","    sample = {\n","        'gif_embeddings': gif_features,\n","        'question_embeddings': question_features,\n","        'answer': answers,\n","        'url': gif_url,\n","        'question': question\n","    }\n","\n","    return sample\n"]},{"cell_type":"markdown","metadata":{},"source":["# INFERENCE FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-03T07:28:47.750129Z","iopub.status.busy":"2024-10-03T07:28:47.749112Z","iopub.status.idle":"2024-10-03T07:28:49.277866Z","shell.execute_reply":"2024-10-03T07:28:49.276704Z","shell.execute_reply.started":"2024-10-03T07:28:47.750058Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["image_features shape: torch.Size([1, 768])\n","question_features shape: torch.Size([1, 768])\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["what does the guy kick a dog and then hits him in the crotch with a rake ?\n","['dog']\n","https://33.media.tumblr.com/9cd7ae01d3187758321523d08fc60db4/tumblr_njrzdxnmrz1tgetb4o1_250.gif\n","dog <END>\n"]}],"source":["def inference(n, model, tokenizer, device):\n","    \"\"\"\n","    Runs inference on the model using the data at index `n`, returning the generated answer, the original question,\n","    and the URL of the GIF.\n","\n","    Args:\n","    n (int): The index of the data point to use for inference.\n","    model (torch.nn.Module): The VQA model used for generating answers.\n","    tokenizer (GPT2Tokenizer): Tokenizer for decoding the generated answer tokens.\n","    device (torch.device): The device (CPU or GPU) to run the inference on.\n","\n","    Returns:\n","    dict: A dictionary containing:\n","        - 'question': The input question.\n","        - 'generated_answers': The model-generated answers.\n","        - 'gif_url': The URL of the GIF.\n","        - 'expected_answer': The actual answer from the dataset.\n","    \"\"\"\n","    \n","    # Fetch the data for the given index `n`\n","    batch = getitem(n)\n","    \n","    # Extract image features, question features, and the answer from the batch\n","    image_features, question_features, answers = batch['gif_embeddings'], batch['question_embeddings'], batch['answer']\n","    image_features = image_features.unsqueeze(0).to(device)\n","    question_features = question_features.unsqueeze(0).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        logits, gen_seq = model(image_features, question_features)\n","    generated_answers = [tokenizer.decode(g, skip_special_tokens=True) for g in gen_seq]\n","    \n","    # Create a dictionary to return the relevant information\n","    result = {\n","        'question': batch['question'],\n","        'generated_answers': generated_answers,\n","        'gif_url': batch['url'],\n","        'expected_answer': answers\n","    }\n","    \n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T18:46:14.059407Z","iopub.status.busy":"2024-09-29T18:46:14.058452Z","iopub.status.idle":"2024-09-29T18:46:15.542603Z","shell.execute_reply":"2024-09-29T18:46:15.541230Z","shell.execute_reply.started":"2024-09-29T18:46:14.059342Z"},"trusted":true},"outputs":[],"source":["# Initialize the tokenizer and device\n","tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Choose a random index for inference\n","n = np.random.randint(0, 18000)\n","result = inference(n, model, tokenizer, device)\n","print(\"Question:\", result['question'])\n","print(\"Generated Answer:\", result['generated_answers'])\n","print(\"GIF URL:\", result['gif_url'])\n","print(\"Expected Answer:\", result['expected_answer'])\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5784662,"sourceId":9504399,"sourceType":"datasetVersion"},{"datasetId":5789247,"sourceId":9510865,"sourceType":"datasetVersion"},{"datasetId":5789325,"sourceId":9510961,"sourceType":"datasetVersion"},{"sourceId":198707724,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
