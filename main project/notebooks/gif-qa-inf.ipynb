{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9504399,"sourceType":"datasetVersion","datasetId":5784662},{"sourceId":9510865,"sourceType":"datasetVersion","datasetId":5789247},{"sourceId":9510961,"sourceType":"datasetVersion","datasetId":5789325},{"sourceId":198707724,"sourceType":"kernelVersion"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WRITING THE MODEL HERE FOR LOADING WEIGHTS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nclass VQAModel(nn.Module):\n    def __init__(self):\n        super(VQAModel, self).__init__()\n        self.gpt2_model = GPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\") \n        self.project_down = nn.Linear(768*2, 768)\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n\n    def forward(self, image_features, question_features):\n        print(\"image_features shape:\", image_features.shape)\n        print(\"question_features shape:\", question_features.shape)\n        combined_features = torch.cat((image_features, question_features), dim=-1)\n        combined_features = self.project_down(combined_features)\n        combined_features = combined_features.unsqueeze(1) \n        outputs = self.gpt2_model(inputs_embeds=combined_features)\n        logits = outputs.logits\n        eos_token_id = self.gpt2_tokenizer.encode(\"<END>\", add_prefix_space=True)[0]\n        generated_sequence = self.gpt2_model.generate(inputs_embeds=combined_features, \n                                                      max_length=2, \n                                                      pad_token_id=eos_token_id, \n                                                      repetition_penalty=5.7,\n                                                      temperature=0.9,\n                                                      eos_token_id=eos_token_id)\n        return logits, generated_sequence","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:43:25.578222Z","iopub.execute_input":"2024-10-03T07:43:25.578815Z","iopub.status.idle":"2024-10-03T07:43:25.587952Z","shell.execute_reply.started":"2024-10-03T07:43:25.578775Z","shell.execute_reply":"2024-10-03T07:43:25.587015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch  # PyTorch library for tensor computations and deep learning\nimport torch.nn as nn  # Neural network modules and layers\nimport torch.optim as optim  # Optimization algorithms\nfrom tqdm import tqdm  # For displaying progress bars during iterations\n\n# -----------------------------------------------------------------------------------\n# Model Loading and Preparation\n# -----------------------------------------------------------------------------------\n\n# Determine the computing device: use GPU ('cuda') if available, else fallback to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the VQA (Visual Question Answering) model and move it to the selected device\nmodel = VQAModel().to(device)\n\n# Wrap the model with DataParallel to utilize multiple GPUs if available\n# DataParallel allows parallel processing across multiple GPUs to speed up computations\nmodel = torch.nn.DataParallel(model)\n\n# -----------------------------------------------------------------------------------\n# Loading Pre-trained Model Weights\n# -----------------------------------------------------------------------------------\n\n# Define the path to the pre-trained model's state dictionary\nmodel_path = '/kaggle/input/open-ended-vqa-qa/best_model.pth'\n\n# Load the state dictionary from the specified path\n# torch.load handles loading the saved state dictionary\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\n# This updates the model's parameters with the pre-trained weights\nmodel.load_state_dict(state_dict)\n\n# Extract the underlying model from DataParallel wrapper\n# When using DataParallel, the actual model is accessible via the `.module` attribute\nmodel_without_dp = model.module\n\n# -----------------------------------------------------------------------------------\n# Saving the Model's State Dictionary Without DataParallel\n# -----------------------------------------------------------------------------------\n\n# Move the model to CPU to ensure compatibility when loading without GPU\n# This is useful for inference on machines without CUDA support\nmodel_without_dp.cpu()\n\n# Define the path where the CPU-compatible state dictionary will be saved\ncpu_model_path = '/kaggle/working/model_cpu.pth'\n\n# Save the state dictionary of the model without DataParallel\n# This allows for easier loading in environments that do not use multiple GPUs\ntorch.save(model_without_dp.state_dict(), cpu_model_path)\n\n# -----------------------------------------------------------------------------------\n# Summary\n# -----------------------------------------------------------------------------------\n\n\"\"\"\nThis script performs the following operations:\n\n1. **Device Configuration**:\n    - Checks if a GPU is available and sets the computation device accordingly.\n\n2. **Model Initialization**:\n    - Instantiates the `VQAModel` and moves it to the selected device.\n    - Wraps the model with `DataParallel` to enable parallel processing across multiple GPUs, enhancing computational efficiency.\n\n3. **Loading Pre-trained Weights**:\n    - Loads a pre-trained model's state dictionary from a specified path.\n    - Updates the initialized model's parameters with these pre-trained weights.\n\n4. **Preparing Model for CPU Inference**:\n    - Extracts the underlying model from the `DataParallel` wrapper to remove dependencies on multiple GPUs.\n    - Moves the model to CPU to ensure compatibility in environments without CUDA support.\n    - Saves the CPU-compatible state dictionary to a designated path for future inference or deployment.\n\n**Use Case**:\nThis script is particularly useful when you have a model trained on a multi-GPU setup and wish to deploy or perform inference on a single GPU or CPU environment. By saving a CPU-compatible version of the model's state dictionary, you ensure broader compatibility and ease of deployment across different hardware configurations.\n\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:28:25.483475Z","iopub.execute_input":"2024-10-03T07:28:25.484008Z","iopub.status.idle":"2024-10-03T07:28:34.549716Z","shell.execute_reply.started":"2024-10-03T07:28:25.483970Z","shell.execute_reply":"2024-10-03T07:28:34.548862Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c04d201da84c3ea32e92dde7a19708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370538a4a6824ad2ab05cc96d07f6c33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18390ce8bdf54015bcecebb7b28a8c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0d6f2b058c4afaafab842dcf6a2550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d32d608403a4dd3bedaedf69e9b6880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ccc0560d3245b08a782ce0ccdd1fec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18fa81da8c2a4d4e91c05a76096cfe88"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/tmp/ipykernel_30/1987625107.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch  # PyTorch library for tensor computations and deep learning\n\n# -----------------------------------------------------------------------------------\n# Model Loading and Preparation\n# -----------------------------------------------------------------------------------\n\n# Determine the computing device: use GPU ('cuda') if available, else fallback to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Initialize the VQA (Visual Question Answering) model and move it to the selected device\n# Ensure that the VQAModel class is defined elsewhere in your codebase\nmodel = VQAModel().to(device)\nprint(\"VQAModel initialized and moved to device.\")\n\n# Define the path to the pre-trained model's state dictionary\nmodel_path = '/kaggle/working/model_cpu.pth'\nprint(f\"Loading model state dictionary from: {model_path}\")\n\n# Load the state dictionary from the specified path\n# torch.load handles loading the saved state dictionary\n# If the model was saved on a GPU and you're loading it on a CPU, use map_location\nstate_dict = torch.load(model_path, map_location=device)\n\n# Load the state dictionary into the model\n# This updates the model's parameters with the pre-trained weights\nmodel.load_state_dict(state_dict)\nprint(\"Model state dictionary loaded successfully.\")\n\n# -----------------------------------------------------------------------------------\n# Summary\n# -----------------------------------------------------------------------------------\n\n\"\"\"\nThis script performs the following operations:\n\n1. **Device Configuration**:\n    - Checks if a GPU is available and sets the computation device accordingly.\n    - Prints out the device being used for computations.\n\n2. **Model Initialization**:\n    - Instantiates the `VQAModel` and moves it to the selected device (GPU or CPU).\n    - Prints a confirmation message upon successful initialization.\n\n3. **Loading Pre-trained Weights**:\n    - Specifies the file path to the saved state dictionary (`.pth` file) of the pre-trained model.\n    - Loads the state dictionary from the specified path, ensuring compatibility with the selected device.\n    - Updates the initialized model's parameters with these pre-trained weights.\n    - Prints a confirmation message upon successful loading of the state dictionary.\n\n**Use Case**:\nThis script is particularly useful when you have a pre-trained VQA model saved as a state dictionary and wish to load it for further inference, evaluation, or continued training. By specifying the device dynamically, the script ensures flexibility across different hardware setups without manual intervention.\n\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:28:34.551124Z","iopub.execute_input":"2024-10-03T07:28:34.551485Z","iopub.status.idle":"2024-10-03T07:28:35.578822Z","shell.execute_reply.started":"2024-10-03T07:28:34.551449Z","shell.execute_reply":"2024-10-03T07:28:35.577819Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/552269125.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pickle\n\ngif_feat_dic = \"/kaggle/input/cleaned-gif-embeddings/combined_file.pkl\"\npickle_dataset = \"/kaggle/input/cleaned-gif-embeddings/questions_with_gpt2_embeddings_using_gpu_cleaned.pkl\"\n\nwith open(gif_feat_dic, 'rb') as f:\n    gif_feat_dict = pickle.load(f)\ntgif_frame = pd.read_pickle(pickle_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:44:15.831693Z","iopub.execute_input":"2024-10-03T07:44:15.832087Z","iopub.status.idle":"2024-10-03T07:44:16.993361Z","shell.execute_reply.started":"2024-10-03T07:44:15.832046Z","shell.execute_reply":"2024-10-03T07:44:16.992493Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(io.BytesIO(b))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# WRITING A FUNCTION TO GET THE VDEO EMBEDDINGS AND TEXT EMBEDDINGS ","metadata":{}},{"cell_type":"code","source":"def getitem(idx):\n    \"\"\"\n    Retrieves a sample from the tgif_frame dataset given an index and returns the relevant features \n    such as GIF embeddings, question features, and answers.\n\n    Args:\n    idx (int): The index of the row in the tgif_frame dataset to retrieve the sample.\n\n    Returns:\n    dict: A dictionary containing the following:\n        - 'gif_embeddings': Precomputed GIF embeddings from gif_feat_dict.\n        - 'question_embeddings': Tensor containing the question features.\n        - 'answer': The corresponding answer for the question, appended with \"<END>\" token.\n        - 'url': The URL of the GIF.\n        - 'question': The textual question.\n    \"\"\"\n    \n    # Retrieve the GIF name by adding '.gif' extension to the value from the second column\n    gif_name = tgif_frame.iloc[idx, 1] + '.gif'\n    \n    # Extract the question features (assumed to be a NumPy array) and convert it to a PyTorch tensor\n    question_features = torch.from_numpy(tgif_frame.iloc[idx, 4])\n    \n    # Retrieve the answer from the third column, and append the \"<END>\" token for sequence processing\n    answers = tgif_frame.iloc[idx, 3] + \" <END>\"\n    \n    # Get the precomputed GIF embeddings from a dictionary using the gif_name as the key\n    gif_features = gif_feat_dict[gif_name]\n    \n    # Retrieve the URL of the GIF from the first column\n    gif_url = tgif_frame.iloc[idx, 0]\n    \n    # Get the textual question from the second column\n    question = tgif_frame.iloc[idx, 2]\n\n    # Construct and return the sample dictionary with the retrieved features\n    sample = {\n        'gif_embeddings': gif_features,\n        'question_embeddings': question_features,\n        'answer': answers,\n        'url': gif_url,\n        'question': question\n    }\n\n    return sample\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:28:45.207330Z","iopub.execute_input":"2024-10-03T07:28:45.208080Z","iopub.status.idle":"2024-10-03T07:28:45.214488Z","shell.execute_reply.started":"2024-10-03T07:28:45.208036Z","shell.execute_reply":"2024-10-03T07:28:45.213319Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# INFERENCE FUNCTION","metadata":{}},{"cell_type":"code","source":"def inference(n, model, tokenizer, device):\n    \"\"\"\n    Runs inference on the model using the data at index `n`, returning the generated answer, the original question,\n    and the URL of the GIF.\n\n    Args:\n    n (int): The index of the data point to use for inference.\n    model (torch.nn.Module): The VQA model used for generating answers.\n    tokenizer (GPT2Tokenizer): Tokenizer for decoding the generated answer tokens.\n    device (torch.device): The device (CPU or GPU) to run the inference on.\n\n    Returns:\n    dict: A dictionary containing:\n        - 'question': The input question.\n        - 'generated_answers': The model-generated answers.\n        - 'gif_url': The URL of the GIF.\n        - 'expected_answer': The actual answer from the dataset.\n    \"\"\"\n    \n    # Fetch the data for the given index `n`\n    batch = getitem(n)\n    \n    # Extract image features, question features, and the answer from the batch\n    image_features, question_features, answers = batch['gif_embeddings'], batch['question_embeddings'], batch['answer']\n    \n    # Unsqueeze the features to add a batch dimension and move them to the specified device\n    image_features = image_features.unsqueeze(0).to(device)\n    question_features = question_features.unsqueeze(0).to(device)\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Forward pass through the model to get logits and the generated sequence of tokens\n    with torch.no_grad():\n        logits, gen_seq = model(image_features, question_features)\n    \n    # Decode the generated token sequences into human-readable strings\n    generated_answers = [tokenizer.decode(g, skip_special_tokens=True) for g in gen_seq]\n    \n    # Create a dictionary to return the relevant information\n    result = {\n        'question': batch['question'],\n        'generated_answers': generated_answers,\n        'gif_url': batch['url'],\n        'expected_answer': answers\n    }\n    \n    return result\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T07:28:47.749112Z","iopub.execute_input":"2024-10-03T07:28:47.750129Z","iopub.status.idle":"2024-10-03T07:28:49.277866Z","shell.execute_reply.started":"2024-10-03T07:28:47.750058Z","shell.execute_reply":"2024-10-03T07:28:49.276704Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"image_features shape: torch.Size([1, 768])\nquestion_features shape: torch.Size([1, 768])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"what does the guy kick a dog and then hits him in the crotch with a rake ?\n['dog']\nhttps://33.media.tumblr.com/9cd7ae01d3187758321523d08fc60db4/tumblr_njrzdxnmrz1tgetb4o1_250.gif\ndog <END>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the tokenizer and device\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Choose a random index for inference\nn = np.random.randint(0, 18000)\n\n# Perform inference\nresult = inference(n, model, tokenizer, device)\n\n# Print the outputs\nprint(\"Question:\", result['question'])\nprint(\"Generated Answer:\", result['generated_answers'])\nprint(\"GIF URL:\", result['gif_url'])\nprint(\"Expected Answer:\", result['expected_answer'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T18:46:14.058452Z","iopub.execute_input":"2024-09-29T18:46:14.059407Z","iopub.status.idle":"2024-09-29T18:46:15.542603Z","shell.execute_reply.started":"2024-09-29T18:46:14.059342Z","shell.execute_reply":"2024-09-29T18:46:15.541230Z"},"trusted":true},"execution_count":1,"outputs":[]}]}