{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9504374,"sourceType":"datasetVersion","datasetId":5779610},{"sourceId":9522747,"sourceType":"datasetVersion","datasetId":5794664}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# THE GIVEN CODE SHOWS THE PROCESS OF GENERATING QUESTION EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom tqdm import tqdm \n\n# -----------------------------------------------------------------------------------\n# Configuration and Setup\n# -----------------------------------------------------------------------------------\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nnum_gpus = torch.cuda.device_count()\nprint(f\"Number of GPUs available: {num_gpus}\")\n\n# -----------------------------------------------------------------------------------\n# Loading the Pre-trained GPT-2 Model and Tokenizer\n# -----------------------------------------------------------------------------------\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n\n\nnew_pad_token = \"[PAD]\"\n\n\nif new_pad_token not in tokenizer.get_vocab():\n    tokenizer.add_special_tokens({'pad_token': new_pad_token})\n    print(f\"Added new pad token: {new_pad_token}\")\nmodel = GPT2Model.from_pretrained(\"openai-community/gpt2-medium\")\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\nif num_gpus > 1:\n    model = torch.nn.DataParallel(model)\n    print(\"Model wrapped with DataParallel for multi-GPU usage.\")\n\n# Set the model to evaluation mode to disable dropout and other training-specific layers\nmodel.eval()\n\n# -----------------------------------------------------------------------------------\n# Function Definitions\n# -----------------------------------------------------------------------------------\n\ndef generate_embeddings_batch(texts, tokenizer, model, device, max_length=64):\n    \"\"\"\n    Generates embeddings for a batch of texts using the GPT-2 model.\n\n    Args:\n        texts (list of str): List of text strings (e.g., questions) to generate embeddings for.\n        tokenizer (GPT2Tokenizer): Tokenizer corresponding to the GPT-2 model.\n        model (GPT2Model): Pre-trained GPT-2 model for generating embeddings.\n        device (torch.device): The device (CPU or GPU) to perform computations on.\n        max_length (int, optional): Maximum sequence length for tokenization. Defaults to 64.\n\n    Returns:\n        numpy.ndarray: Array of embeddings with shape (batch_size, hidden_size).\n    \"\"\"\n    # Tokenize the batch of texts with padding and truncation\n    inputs = tokenizer(\n        texts,\n        return_tensors='pt',\n        max_length=max_length,\n        truncation=True,\n        padding='max_length'\n    )\n    \n    # Move tokenized inputs to the designated device\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    \n    with torch.no_grad():  # Disable gradient calculations for efficiency\n        outputs = model(**inputs)       \n        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)\n        embeddings = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    \n    return embeddings\n\n# -----------------------------------------------------------------------------------\n# Parameters and Data Loading\n# -----------------------------------------------------------------------------------\n\n\nbatch_size = 128 \n\n# Load the dataset from a CSV file into a Pandas DataFrame\ndf = pd.read_csv(\"/kaggle/input/tgif-qna-descriptions-38k/updated_final_dataframe_complete.csv\")\nembeddings = []\n\n# -----------------------------------------------------------------------------------\n# Generating Embeddings for the Dataset\n# -----------------------------------------------------------------------------------\n\n# Iterate over the DataFrame in batches to efficiently process large datasets\nfor i in tqdm(range(0, len(df), batch_size), desc=\"Generating Embeddings\"):\n    batch_df = df.iloc[i:i+batch_size]\n    batch_texts = batch_df['question']\n    batch_embeddings = generate_embeddings_batch(batch_texts.tolist(), tokenizer, model, device)\n    embeddings.extend(batch_embeddings)\n\n# -----------------------------------------------------------------------------------\n# Saving the Embeddings\n# -----------------------------------------------------------------------------------\n\n# Add the generated embeddings as a new column in the DataFrame\ndf['question_embedding'] = embeddings\ndf.to_pickle('updated_final_df_with_q_embeddings.pkl')","metadata":{},"execution_count":null,"outputs":[]}]}
