{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9522747,"sourceType":"datasetVersion","datasetId":5794664},{"sourceId":199032368,"sourceType":"kernelVersion"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --quiet av sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T10:38:24.056554Z","iopub.execute_input":"2024-10-01T10:38:24.056846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CUSTOM DATASET AND DATALOADER FOR OUR DATA","metadata":{}},{"cell_type":"code","source":"import os\nimport av  # PyAV for handling video files\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport torch\nimport requests  # For downloading GIFs\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, TimesformerModel\nfrom transformers import GPT2Tokenizer\n\nclass VQAGIFDataset(Dataset):\n    \"\"\"\n    A custom Dataset class for handling Video QA (VQA) GIF data.\n\n    This dataset handles downloading GIFs, processing video frames, and preparing\n    question-answer pairs for training or evaluation in a VQA system.\n\n    Args:\n        pickle_dataset (str): Path to the pickled DataFrame containing the dataset.\n    \"\"\"\n    def __init__(self, pickle_dataset):  # transform=None\n        # Load the dataset from a pickle file\n        self.tgif_frame = pd.read_pickle(pickle_dataset)\n        \n        # Initialize the image processor from the pre-trained VideoMAE model\n        self.preprocess = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n        \n        # Initialize the GPT-2 tokenizer\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n        \n        # Define a new padding token\n        new_pad_token = \"[PAD]\"\n        \n        # Add the new padding token to the tokenizer's vocabulary if it's not already present\n        if new_pad_token not in self.gpt2_tokenizer.get_vocab():\n            self.gpt2_tokenizer.add_special_tokens({'pad_token': new_pad_token})\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        Returns:\n            int: Number of samples.\n        \"\"\"\n        return len(self.tgif_frame)\n    \n    def download_gif(self, gif_url, output_path):\n        \"\"\"\n        Downloads a GIF from a given URL and saves it to the specified path.\n\n        Args:\n            gif_url (str): URL of the GIF to download.\n            output_path (str): Local path where the GIF will be saved.\n\n        Returns:\n            bool: True if download was successful, False otherwise.\n        \"\"\"\n        response = requests.get(gif_url, stream=True)\n        if response.status_code == 200:\n            with open(output_path, 'wb') as f:\n                f.write(response.content)\n            return True\n        return False\n\n    def read_video_pyav(self, container, indices):\n        \"\"\"\n        Decodes video frames from a PyAV container based on specified indices.\n\n        Args:\n            container (av.container.input.InputContainer): PyAV container for the video.\n            indices (list of int): Frame indices to extract.\n\n        Returns:\n            numpy.ndarray: Array of extracted frames in RGB format.\n        \"\"\"\n        frames = []\n        container.seek(0)  # Seek to the beginning of the video\n        start_index = indices[0]\n        end_index = indices[-1]\n        \n        # Iterate through decoded frames and collect those at the specified indices\n        for i, frame in enumerate(container.decode(video=0)):\n            if i > end_index:\n                break  # Stop if we've passed the last desired frame\n            if i >= start_index and i in indices:\n                frames.append(frame)\n        \n        # Convert frames to numpy arrays in RGB format and stack them\n        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n    def sample_frame_indices(self, clip_len, frame_sample_rate, seg_len):\n        \"\"\"\n        Samples a set of frame indices from the video for processing.\n\n        Args:\n            clip_len (int): Number of frames to sample.\n            frame_sample_rate (float): Sampling rate (e.g., 0.5 means every other frame).\n            seg_len (int): Total number of frames in the video.\n\n        Returns:\n            numpy.ndarray: Array of sampled frame indices.\n        \"\"\"\n        converted_len = int(clip_len * frame_sample_rate)\n        \n        # Randomly choose an end index ensuring the clip fits within the segment length\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        \n        # Generate evenly spaced indices between start and end\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        \n        # Ensure indices are within valid range and convert to integers\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        \n        return indices\n    \n    def process_gifs(self, gif_url):\n        \"\"\"\n        Downloads and processes a GIF to extract video frames.\n\n        Args:\n            gif_url (str): URL of the GIF to process.\n\n        Returns:\n            list of numpy.ndarray: List of processed video frames.\n        \"\"\"\n        embeddings_dict = {}\n        gifs_checked = 0\n        true_gifs = []\n        download_folder = \"/kaggle/working/gifs\"\n\n        # Create the download folder if it doesn't exist\n        if not os.path.exists(download_folder):\n            os.makedirs(download_folder)\n\n        # Extract the GIF filename from the URL\n        gif_name = gif_url.split('/')[-1]\n        gif_path = os.path.join(download_folder, gif_name)\n\n        # Download the GIF\n        if self.download_gif(gif_url, gif_path):\n            gifs_checked += 1\n\n            # Open the downloaded GIF using PyAV\n            container = av.open(gif_path)\n            total_frames = container.streams.video[0].frames\n\n            # Sample 8 frames from the GIF\n            indices = self.sample_frame_indices(clip_len=8, frame_sample_rate=0.5, seg_len=total_frames)\n            video = self.read_video_pyav(container, indices)\n\n            # Close the container to release resources\n            container.close()\n\n            # Delete the GIF file after processing to save space\n            if os.path.exists(gif_path):\n                os.remove(gif_path)  # Deletes the downloaded GIF file\n\n            return list(video)\n        else:\n            # Handle download failure (optional: log or raise an exception)\n            return []\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a single sample from the dataset.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            dict: A dictionary containing processed GIF frames, question embeddings, and answer tokens.\n        \"\"\"\n        # Get the GIF URL from the dataset\n        gif_url = self.tgif_frame.iloc[idx, 0]\n        \n        # Process the GIF to extract video frames\n        video_list = self.process_gifs(gif_url)\n        \n        # Preprocess the video frames using the image processor\n        inputs = self.preprocess(video_list, return_tensors=\"pt\")\n        pixel_values = inputs.get('pixel_values')\n        pixel_values = pixel_values.squeeze(0)  # Remove batch dimension\n        \n        # Extract question features and answer from the dataset\n        question_features = self.tgif_frame.iloc[idx, 4]\n        answers = self.tgif_frame.iloc[idx, 3] + \" <END>\"  # Append end token to the answer\n        \n        # Tokenize the answer using GPT-2 tokenizer\n        answer_id = self.gpt2_tokenizer(\n            answers,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=\"max_length\",\n            max_length=37\n        ).input_ids.squeeze(0)  # Remove batch dimension\n        \n        # Create a sample dictionary\n        sample = {\n            'processed_gifs': pixel_values,          # Tensor of processed GIF frames\n            'question_embeddings': question_features, # Precomputed question embeddings\n            'answer': answer_id                      # Tokenized answer\n        }\n        \n        return sample\n\n\ndef get_loaders(\n    pickle_dataset='/kaggle/input/embedding-q-all/updated_final_df_with_q_embeddings.pkl',\n    batch_size=32,\n    split_ratio=(0.9, 0.09, 0.01)\n):\n    \"\"\"\n    Creates and returns DataLoader objects for training, validation, and testing.\n\n    Args:\n        pickle_dataset (str, optional): Path to the pickled dataset. Defaults to\n            '/kaggle/input/embedding-q-all/updated_final_df_with_q_embeddings.pkl'.\n        batch_size (int, optional): Number of samples per batch. Defaults to 32.\n        split_ratio (tuple, optional): Ratios for train, validation, and test splits.\n            Should sum to 1. Defaults to (0.9, 0.09, 0.01).\n\n    Returns:\n        tuple: DataLoader objects for training, validation, and testing.\n    \"\"\"\n    # Ensure that the split ratios sum to 1\n    assert sum(split_ratio) == 1, \"Split ratios should sum to 1.\"\n\n    # Initialize the custom dataset\n    dataset = VQAGIFDataset(pickle_dataset)  # , transform=transform\n    \n    # Calculate sizes for each split\n    total_size = len(dataset)\n    train_size = int(split_ratio[0] * total_size)\n    val_size = int(split_ratio[1] * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split the dataset into training, validation, and testing\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n        dataset, [train_size, val_size, test_size]\n    )\n    \n    # Save the test dataset for later use (optional)\n    torch.save(test_dataset, '/kaggle/working/test_dataset.pth')\n    \n    # Create DataLoader objects for each split\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREPARING THE MODEL","metadata":{}},{"cell_type":"code","source":"import os\nimport av  # PyAV for handling video files\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport torch\nimport requests  # For downloading GIFs\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, TimesformerModel\nfrom transformers import GPT2Tokenizer\n\nclass VQAGIFDataset(Dataset):\n    \"\"\"\n    A custom Dataset class for handling Video QA (VQA) GIF data.\n\n    This dataset handles downloading GIFs, processing video frames, and preparing\n    question-answer pairs for training or evaluation in a VQA system.\n\n    Args:\n        pickle_dataset (str): Path to the pickled DataFrame containing the dataset.\n    \"\"\"\n    def __init__(self, pickle_dataset):  # transform=None\n        # Load the dataset from a pickle file\n        self.tgif_frame = pd.read_pickle(pickle_dataset)\n        \n        # Initialize the image processor from the pre-trained VideoMAE model\n        self.preprocess = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n        \n        # Initialize the GPT-2 tokenizer\n        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n        \n        # Define a new padding token\n        new_pad_token = \"[PAD]\"\n        \n        # Add the new padding token to the tokenizer's vocabulary if it's not already present\n        if new_pad_token not in self.gpt2_tokenizer.get_vocab():\n            self.gpt2_tokenizer.add_special_tokens({'pad_token': new_pad_token})\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of samples in the dataset.\n\n        Returns:\n            int: Number of samples.\n        \"\"\"\n        return len(self.tgif_frame)\n    \n    def download_gif(self, gif_url, output_path):\n        \"\"\"\n        Downloads a GIF from a given URL and saves it to the specified path.\n\n        Args:\n            gif_url (str): URL of the GIF to download.\n            output_path (str): Local path where the GIF will be saved.\n\n        Returns:\n            bool: True if download was successful, False otherwise.\n        \"\"\"\n        response = requests.get(gif_url, stream=True)\n        if response.status_code == 200:\n            with open(output_path, 'wb') as f:\n                f.write(response.content)\n            return True\n        return False\n\n    def read_video_pyav(self, container, indices):\n        \"\"\"\n        Decodes video frames from a PyAV container based on specified indices.\n\n        Args:\n            container (av.container.input.InputContainer): PyAV container for the video.\n            indices (list of int): Frame indices to extract.\n\n        Returns:\n            numpy.ndarray: Array of extracted frames in RGB format.\n        \"\"\"\n        frames = []\n        container.seek(0)  # Seek to the beginning of the video\n        start_index = indices[0]\n        end_index = indices[-1]\n        \n        # Iterate through decoded frames and collect those at the specified indices\n        for i, frame in enumerate(container.decode(video=0)):\n            if i > end_index:\n                break  # Stop if we've passed the last desired frame\n            if i >= start_index and i in indices:\n                frames.append(frame)\n        \n        # Convert frames to numpy arrays in RGB format and stack them\n        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n    def sample_frame_indices(self, clip_len, frame_sample_rate, seg_len):\n        \"\"\"\n        Samples a set of frame indices from the video for processing.\n\n        Args:\n            clip_len (int): Number of frames to sample.\n            frame_sample_rate (float): Sampling rate (e.g., 0.5 means every other frame).\n            seg_len (int): Total number of frames in the video.\n\n        Returns:\n            numpy.ndarray: Array of sampled frame indices.\n        \"\"\"\n        converted_len = int(clip_len * frame_sample_rate)\n        \n        # Randomly choose an end index ensuring the clip fits within the segment length\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        \n        # Generate evenly spaced indices between start and end\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        \n        # Ensure indices are within valid range and convert to integers\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        \n        return indices\n    \n    def process_gifs(self, gif_url):\n        \"\"\"\n        Downloads and processes a GIF to extract video frames.\n\n        Args:\n            gif_url (str): URL of the GIF to process.\n\n        Returns:\n            list of numpy.ndarray: List of processed video frames.\n        \"\"\"\n        embeddings_dict = {}\n        gifs_checked = 0\n        true_gifs = []\n        download_folder = \"/kaggle/working/gifs\"\n\n        # Create the download folder if it doesn't exist\n        if not os.path.exists(download_folder):\n            os.makedirs(download_folder)\n\n        # Extract the GIF filename from the URL\n        gif_name = gif_url.split('/')[-1]\n        gif_path = os.path.join(download_folder, gif_name)\n\n        # Download the GIF\n        if self.download_gif(gif_url, gif_path):\n            gifs_checked += 1\n\n            # Open the downloaded GIF using PyAV\n            container = av.open(gif_path)\n            total_frames = container.streams.video[0].frames\n\n            # Sample 8 frames from the GIF\n            indices = self.sample_frame_indices(clip_len=8, frame_sample_rate=0.5, seg_len=total_frames)\n            video = self.read_video_pyav(container, indices)\n\n            # Close the container to release resources\n            container.close()\n\n            # Delete the GIF file after processing to save space\n            if os.path.exists(gif_path):\n                os.remove(gif_path)  # Deletes the downloaded GIF file\n\n            return list(video)\n        else:\n            # Handle download failure (optional: log or raise an exception)\n            return []\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieves a single sample from the dataset.\n\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            dict: A dictionary containing processed GIF frames, question embeddings, and answer tokens.\n        \"\"\"\n        # Get the GIF URL from the dataset\n        gif_url = self.tgif_frame.iloc[idx, 0]\n        \n        # Process the GIF to extract video frames\n        video_list = self.process_gifs(gif_url)\n        \n        # Preprocess the video frames using the image processor\n        inputs = self.preprocess(video_list, return_tensors=\"pt\")\n        pixel_values = inputs.get('pixel_values')\n        pixel_values = pixel_values.squeeze(0)  # Remove batch dimension\n        \n        # Extract question features and answer from the dataset\n        question_features = self.tgif_frame.iloc[idx, 4]\n        answers = self.tgif_frame.iloc[idx, 3] + \" <END>\"  # Append end token to the answer\n        \n        # Tokenize the answer using GPT-2 tokenizer\n        answer_id = self.gpt2_tokenizer(\n            answers,\n            return_tensors=\"pt\",\n            truncation=True,\n            padding=\"max_length\",\n            max_length=37\n        ).input_ids.squeeze(0)  # Remove batch dimension\n        \n        # Create a sample dictionary\n        sample = {\n            'processed_gifs': pixel_values,          # Tensor of processed GIF frames\n            'question_embeddings': question_features, # Precomputed question embeddings\n            'answer': answer_id                      # Tokenized answer\n        }\n        \n        return sample\n\n\ndef get_loaders(\n    pickle_dataset='/kaggle/input/embedding-q-all/updated_final_df_with_q_embeddings.pkl',\n    batch_size=32,\n    split_ratio=(0.9, 0.09, 0.01)\n):\n    \"\"\"\n    Creates and returns DataLoader objects for training, validation, and testing.\n\n    Args:\n        pickle_dataset (str, optional): Path to the pickled dataset. Defaults to\n            '/kaggle/input/embedding-q-all/updated_final_df_with_q_embeddings.pkl'.\n        batch_size (int, optional): Number of samples per batch. Defaults to 32.\n        split_ratio (tuple, optional): Ratios for train, validation, and test splits.\n            Should sum to 1. Defaults to (0.9, 0.09, 0.01).\n\n    Returns:\n        tuple: DataLoader objects for training, validation, and testing.\n    \"\"\"\n    # Ensure that the split ratios sum to 1\n    assert sum(split_ratio) == 1, \"Split ratios should sum to 1.\"\n\n    # Initialize the custom dataset\n    dataset = VQAGIFDataset(pickle_dataset)  # , transform=transform\n    \n    # Calculate sizes for each split\n    total_size = len(dataset)\n    train_size = int(split_ratio[0] * total_size)\n    val_size = int(split_ratio[1] * total_size)\n    test_size = total_size - train_size - val_size\n    \n    # Split the dataset into training, validation, and testing\n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n        dataset, [train_size, val_size, test_size]\n    )\n    \n    # Save the test dataset for later use (optional)\n    torch.save(test_dataset, '/kaggle/working/test_dataset.pth')\n    \n    # Create DataLoader objects for each split\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING FUNCTION FOR OUT MODEL","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm  # For displaying progress bars during iteration\nfrom sentence_transformers import SentenceTransformer  # For embedding sentences\nfrom sklearn.metrics.pairwise import cosine_similarity  # For computing similarity between embeddings\n\nfrom transformers import GPT2Tokenizer  # GPT-2 tokenizer\n\n# -----------------------------------------------------------------------------------\n# Configuration and Setup\n# -----------------------------------------------------------------------------------\n\n# Initialize the GPT-2 tokenizer from the specified pre-trained model\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n\n# Define training hyperparameters\nBATCH_SIZE = 16\nEPOCHS = 2\nLEARNING_RATE = 1e-4\n\n# Define paths for saving models and test data\nMODEL_PATH = '/kaggle/working/best_model.pth'\nMODEL_PATH_2 = '/kaggle/working/10_model.pth'\nMODEL_PATH_3 = '/kaggle/working/15_model.pth'\ntest_data_path = '/kaggle/working/test_data.csv'\n\n# Determine the computing device: use GPU if available, else fallback to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize data loaders for training, validation, and testing\ntrain_loader, val_loader, test_loader = get_loaders(batch_size=BATCH_SIZE)\n\n# Initialize the VQA model and move it to the designated device\nmodel = VQAModel().to(device)\n\n# If multiple GPUs are available, wrap the model with DataParallel for parallel processing\nmodel = torch.nn.DataParallel(model)\n\n# Initialize the SentenceTransformer model for validation embeddings\nval_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Define the loss function as Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize the optimizer as Adam with the specified learning rate\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# -----------------------------------------------------------------------------------\n# Training and Validation Functions\n# -----------------------------------------------------------------------------------\n\ndef train(model, dataloader, criterion, optimizer, device):\n    \"\"\"\n    Trains the model for one epoch.\n\n    Args:\n        model (torch.nn.Module): The VQA model to train.\n        dataloader (DataLoader): DataLoader for the training dataset.\n        criterion (torch.nn.Module): Loss function.\n        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n        device (torch.device): The device to perform computations on.\n\n    Returns:\n        float: The average training loss over the epoch.\n    \"\"\"\n    model.train()  # Set the model to training mode\n    running_loss = 0.0  # Initialize running loss\n\n    # Iterate over batches in the training DataLoader with a progress bar\n    for batch in tqdm(train_loader, desc=\"Training\"): \n        # Extract data from the batch\n        images = batch['processed_gifs'].to(device)\n        question_features = batch['question_embeddings'].to(device)\n        answers = batch['answer'].to(device)\n        \n        optimizer.zero_grad()  # Reset gradients\n\n        # Forward pass: compute logits from the model\n        logits, _ = model(images, question_features)\n        \n        # Align logits and answers dimensions if necessary\n        if logits.size(1) < answers.size(1):\n            answers = answers[:, :logits.size(1)]\n        elif logits.size(1) > answers.size(1):\n            logits = logits[:, :answers.size(1)]\n        \n        # Reshape logits and answers for loss computation\n        logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n        answers_reshaped = answers.contiguous().view(-1)\n        \n        # Compute loss\n        loss = criterion(logits_reshaped, answers_reshaped)\n        loss.backward()  # Backpropagate loss\n        optimizer.step()  # Update model parameters\n\n        running_loss += loss.item()  # Accumulate loss\n\n    # Calculate average loss over the epoch\n    avg_loss = running_loss / len(dataloader)\n    return avg_loss\n\n\ndef validate(model, dataloader, criterion, tokenizer, device):\n    \"\"\"\n    Validates the model on the validation dataset.\n\n    Args:\n        model (torch.nn.Module): The VQA model to validate.\n        dataloader (DataLoader): DataLoader for the validation dataset.\n        criterion (torch.nn.Module): Loss function.\n        tokenizer (GPT2Tokenizer): Tokenizer for decoding answers.\n        device (torch.device): The device to perform computations on.\n\n    Returns:\n        tuple:\n            float: The average validation loss.\n            float: The average cosine similarity between reference and generated answers.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    running_loss = 0.0  # Initialize running loss\n    similarity_scores = []  # List to store cosine similarity scores\n    \n    with torch.no_grad():  # Disable gradient computation\n        # Iterate over batches in the validation DataLoader with a progress bar\n        for batch in tqdm(dataloader, desc=\"Validating\"):\n            # Extract data from the batch\n            images = batch['processed_gifs'].to(device)\n            question_features = batch['question_embeddings'].to(device)\n            answer_tokens = batch['answer'].to(device)\n\n            # Forward pass: compute logits and generated sequences from the model\n            logits, gen_seq = model(images, question_features)\n            \n            # Decode reference answers and generated sequences to strings\n            reference_answers = [tokenizer.decode(a, skip_special_tokens=True) for a in answer_tokens]\n            generated_answers = [tokenizer.decode(g, skip_special_tokens=True) for g in gen_seq]\n            \n            # Encode reference and generated answers using the validation SentenceTransformer model\n            reference_embeddings = val_model.encode(reference_answers, convert_to_tensor=False)\n            model_embeddings = val_model.encode(generated_answers, convert_to_tensor=False)\n\n            # Compute cosine similarity for each pair of reference and generated embeddings\n            for ref_emb, mod_emb in zip(reference_embeddings, model_embeddings):\n                similarity = cosine_similarity([ref_emb], [mod_emb])[0][0]\n                similarity_scores.append(similarity)\n            \n            # Align logits and answer_tokens dimensions if necessary for loss computation\n            if logits.size(1) < answer_tokens.size(1):\n                answer_tokens = answer_tokens[:, :logits.size(1)]\n            elif logits.size(1) > answer_tokens.size(1):\n                logits = logits[:, :answer_tokens.size(1)]\n\n            # Reshape logits and answers for loss computation\n            logits_reshaped = logits.contiguous().view(-1, logits.size(-1))\n            answers_reshaped = answer_tokens.contiguous().view(-1)\n\n            # Compute loss\n            loss = criterion(logits_reshaped, answers_reshaped)\n            running_loss += loss.item()  # Accumulate loss\n    \n    # Calculate average loss over the validation dataset\n    avg_loss = running_loss / len(dataloader)\n    \n    # Calculate average cosine similarity, handling the case of no similarity scores\n    average_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0\n\n    return avg_loss, average_similarity\n\n# -----------------------------------------------------------------------------------\n# Training Loop\n# -----------------------------------------------------------------------------------\n\n# Initialize the best validation loss to infinity for tracking improvements\nbest_val_loss = float('inf')\n\n# Iterate over the number of epochs\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n\n    # Train the model for one epoch and retrieve the average training loss\n    train_loss = train(model, train_loader, criterion, optimizer, device)\n\n    # Validate the model and retrieve the average validation loss and similarity\n    val_loss, val_accuracy = validate(model, val_loader, criterion, tokenizer, device)\n    \n    # Display the training and validation metrics\n    print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.2f}%\")\n  \n    # Save the model if the validation loss has improved\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"Model {epoch} saved to {MODEL_PATH}\")\n    \n    # Save intermediate models at specific epochs\n    if epoch == 1:\n        torch.save(model.state_dict(), MODEL_PATH_2)\n        print(f\"Model saved to {MODEL_PATH_2}\")\n    \n    if epoch == 2:\n        torch.save(model.state_dict(), MODEL_PATH_3)\n        print(f\"Model saved to {MODEL_PATH_3}\")\n\n# Indicate that training has completed\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}